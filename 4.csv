"REVIEW"
""
""
""
"Applications of Machine Learning in Cancer Prediction"
"and Prognosis"
"Joseph A. Cruz, David S. Wishart"
"Departments of Biological Science and Computing Science, University of Alberta Edmonton, AB,"
"Canada T6G 2E8"
""
""
"Abstract: Machine learning is a branch of artificial intelligence that employs a variety of statistical, probabilistic and"
"optimization techniques that allows computers to “learn” from past examples and to detect hard-to-discern patterns from"
"large, noisy or complex data sets. This capability is particularly well-suited to medical applications, especially those that"
"depend on complex proteomic and genomic measurements. As a result, machine learning is frequently used in cancer"
"diagnosis and detection. More recently machine learning has been applied to cancer prognosis and prediction. This latter"
"approach is particularly interesting as it is part of a growing trend towards personalized, predictive medicine. In assembling"
"this review we conducted a broad survey of the different types of machine learning methods being used, the types of data"
"being integrated and the performance of these methods in cancer prediction and prognosis. A number of trends are noted,"
"including a growing dependence on protein biomarkers and microarray data, a strong bias towards applications in prostate"
"and breast cancer, and a heavy reliance on “older” technologies such artificial neural networks (ANNs) instead of more"
"recently developed or more easily interpretable machine learning methods. A number of published studies also appear to"
"lack an appropriate level of validation or testing. Among the better designed and validated studies it is clear that machine"
"learning methods can be used to substantially (15-25%) improve the accuracy of predicting cancer susceptibility, recurrence"
"and mortality. At a more fundamental level, it is also evident that machine learning is also helping to improve our basic"
"understanding of cancer development and progression."
""
"Keywords : Cancer, machine learning, prognosis, risk, prediction"
""
""
"Introduction"
"Machine learning is not new to cancer research. Artificial neural networks (ANNs) and decision trees"
"(DTs) have been used in cancer detection and diagnosis for nearly 20 years (Simes 1985; Maclin et al."
"1991; Ciccheti 1992). Today machine learning methods are being used in a wide range of applications"
"ranging from detecting and classifying tumors via X-ray and CRT images (Petricoin and Liotta 2004;"
"Bocchi et al. 2004) to the classification of malignancies from proteomic and genomic (microarray)"
"assays (Zhou et al. 2004; Dettling 2004; Wang et al. 2005). According to the latest PubMed statistics,"
"more than 1500 papers have been published on the subject of machine learning and cancer. However,"
"the vast majority of these papers are concerned with using machine learning methods to identify, classify,"
"detect, or distinguish tumors and other malignancies. In other words machine learning has been used"
"primarily as an aid to cancer diagnosis and detection (McCarthy et al. 2004). It has only been relatively"
"recently that cancer researchers have attempted to apply machine learning towards cancer prediction"
"and prognosis. As a consequence the body of literature in the field of machine learning and cancer"
"prediction/prognosis is relatively small (<120 papers)."
"The fundamental goals of cancer prediction and prognosis are distinct from the goals of cancer detection"
"and diagnosis. In cancer prediction/prognosis one is concerned with three predictive foci: 1) the prediction of"
"cancer susceptibility (i.e. risk assessment); 2) the prediction of cancer recurrence and 3) the prediction of"
"cancer survivability. In the first case, one is trying to predict the likelihood of developing a type of cancer prior"
"to the occurrence of the disease. In the second case one is trying to predict the likelihood of redeveloping"
"cancer after to the apparent resolution of the disease. In the third case one is trying to predict an outcome (life"
"expectancy, survivability, progression, tumor-drug sensitivity) after the diagnosis of the disease. In the latter"
"two situations the success of the prognostic prediction is obviously dependent, in part, on the success or quality"
"of the diagnosis. However a disease prognosis can only come after a medical diagnosis and a prognostic"
"prediction must take into account more than just a simple diagnosis (Hagerty et al. 2005)."
""
""
"Correspondence: David S Wishart, 2-21 Athabasca Hall, University of Alberta, Edmonton, AB Canada. Email:"
"david.wishart@ualberta.ca, Fax: 780-492-1071"
""
"Cancer Informatics 2006: 2 59– 78                                                                                          59"
"Cruz and Wishart"
""
""
"Indeed, a cancer prognosis typically involves        cancer prognoses and predictions improves even"
"multiple physicians from different specialties using     more. However, as the number of parameters we"
"different subsets of biomarkers and multiple             measure grows, so too does the challenge of trying to"
"clinical factors, including the age and general          make sense of all this information."
"health of the patient, the location and type of              In the past, our dependency on macro-scale"
"cancer, as well as the grade and size of the tumor       information (tumor, patient, population, and"
"(Fielding et al. 1992; Cochran 1997; Burke et al.        environmental data) generally kept the numbers"
"2005). Typically histological (cell-based), clinical     of variables small enough so that standard"
"(patient-based) and demographic (population-             statistical methods or even a physician’s own"
"based) information must all be carefully integrated      intuition could be used to predict cancer risks and"
"by the attending physician to come up with a             outcomes. However, with today’s high-throughput"
"reasonable prognosis. Even for the most skilled          diagnostic and imaging technologies we now find"
"clinician, this is not easy to do. Similar challenges    ourselves overwhelmed with dozens or even"
"also exist for both physicians and patients alike        hundreds of molecular, cellular and clinical"
"when it comes to the issues of cancer prevention         parameters. In these situations, human intuition"
"and cancer susceptibility prediction. Family             and standard statistics don’t generally work."
"history, age, diet, weight (obesity), high-risk habits   Instead we must increasingly rely on non-"
"(smoking, heavy drinking), and exposure to               traditional, intensively computational approaches"
"environmental carcinogens (UV radiation, radon,          such as machine learning. The use of computers"
"asbestos, PCBs) all play a role in predicting an         (and machine learning) in disease prediction and"
"individual’s risk for developing cancer (Leenhouts       prognosis is part of a growing trend towards"
"1999; Bach et al. 2003; Gascon et al. 2004; Claus        personalized, predictive medicine (Weston and"
"2001; Domchek et al. 2003). Unfortunately these          Hood 2004). This movement towards predictive"
"conventional          “macro-scale”          clinical,   medicine is important, not only for patients (in"
"environmental and behavioral parameters generally        terms of lifestyle and quality-of-life decisions) but"
"do not provide enough information to make robust         also for physicians (in making treatment decisions)"
"predictions or prognoses. Ideally what is needed         as well as health economists and policy planners"
"is some very specific molecular details about either     (in implementing large scale cancer prevention or"
"the tumor or the patient’s own genetic make-up           cancer treatment policies)."
"(Colozza et al. 2005).                                       Given the growing importance of predictive"
"With the rapid development of genomic (DNA           medicine and the growing reliance on machine"
"sequencing, microarrays), proteomic (protein             learning to make predictions, we believed it would"
"chips, tissue arrays, immuno-histology) and              be of interest to conduct a detailed review of"
"imaging (fMRI, PET, micro-CT) technologies, this         published studies employing machine learning"
"kind of molecular-scale information about patients       methods in cancer prediction and prognosis. The"
"or tumors can now be readily acquired. Molecular         intent is to identify key trends with respect to the"
"biomarkers, such as somatic mutations in certain         types of machine learning methods being used, the"
"genes (p53, BRCA1, BRCA2), the appearance or             types of training data being integrated, the kinds"
"expression of certain tumor proteins (MUC1,              of endpoint predictions being made, the types of"
"HER2, PSA) or the chemical environment of the            cancers being studied and the overall performance"
"tumor (anoxic, hypoxic) have been shown to serve         of these methods in predicting cancer susceptibility"
"as very powerful prognostic or predictive                or patient outcomes. Interestingly, when referring"
"indicators (Piccart et al. 2001; Duffy 2001; Baldus      to cancer prediction and prognosis we found that"
"et al. 2004). More recently, combinations or             most studies were concerned with three"
"patterns of multiple molecular biomarkers have           “predictive” foci or clinical endpoints: 1) the"
"been found to be even more predictive than single        prediction of cancer susceptibility (i.e. risk"
"component tests or readouts (Savage and Gascoyne         assessment); 2) the prediction of cancer recurrence"
"2004; Petricoin and Liotta 2004; Duffy 2005; Vendrell    and 3) the prediction of cancer survivability. We also"
"et al. 2005) If these molecular patterns are combined    found that almost all predictions are made using just"
"with macro-scale clinical data (tumor type, hereditary   four types of input data: genomic data (SNPs,"
"aspects, risk factors), the robustness and accuracy of   mutations, microarrays), proteomic data (specific"
""
""
""
"60                                                                                      Cancer Informatics 2006: 2"
"                                                                                                            Cancer prediction and prognosis"
""
"40"
"kinds of situations, it is important to have a good"
"35                                                             understanding of what machine learning is – and"
"what it isn’t. Machine learning is a branch of"
"30                                                             artificial intelligence research that employs a"
"Number of Papers"
""
""
""
""
"25"
"variety of statistical, probabilistic and optimization"
"tools to “learn” from past examples and to then"
"20                                                             use that prior training to classify new data, identify"
"new patterns or predict novel trends (Mitchell"
"15"
"1997). Machine learning, like statistics, is used to"
"10                                                             analyze and interpret data. Unlike statistics,"
"though, machine learning methods can employ"
"5                                                              Boolean logic (AND, OR, NOT), absolute"
"conditionality (IF, THEN, ELSE), conditional"
"0"
"<1994   1994-95 1996-97 1998-99 2000-01 2002-03 2004-05   probabilities (the probability of X given Y) and"
"Year"
"unconventional optimization strategies to model"
"Figure 1. A histogram showing the steady increase in published               data or classify patterns. These latter methods"
"papers using machine learning methods to predict cancer risk,"
"recurrence and outcome. The data were collected using a variety              actually resemble the approaches humans typically"
"of keyword searches through PubMed, CiteSeer, Google Scholar,                use to learn and classify. Machine learning still"
"Science Citation Index and other online resources. Each bar"
"represents the cumulative total of papers published over a two"
"draws heavily from statistics and probability, but"
"year period. The earliest papers appeared in the early 1990’s.               it is fundamentally more powerful because it allows"
"inferences or decisions to be made that could not"
"protein biomarkers, 2D gel data, mass spectral                               otherwise be made using conventional statistical"
"analyses), clinical data (histology, tumor staging, tumor                    methodologies (Mitchell 1997; Duda et al. 2001)."
"size, age, weight, risk behavior, etc.) or combinations                      For instance, many statistical methods are based"
"of these three. In comparing and evaluating the existing                     on multivariate regression or correlation analysis."
"studies a number of general trends were noted and a                          While generally very powerful, these approaches"
"number of common problems detected. Some of the                              assume that the variables are independent and that"
"more obvious trends include a rapidly growing use of                         data can be modeled using linear combinations of"
"machine learning methods in cancer prediction and                            these variables. When the relationships are non-"
"prognosis (Figure 1), a growing reliance on protein                          linear and the variables are interdependent (or"
"markers and microarray data, a trend towards using                           conditionally dependent) conventional statistics"
"mixed (proteomic + clinical) data, a strong bias                             usually flounders. It is in these situations where"
"towards applications in prostate and breast cancer,                          machine learning tends to shine. Many biological"
"and an unexpected dependency on older                                        systems are fundamentally nonlinear and their"
"technologies such as artificial neural networks                              parameters conditionally dependent. Many simple"
"(ANNs). Among the more commonly noted                                        physical systems are linear and their parameters"
"problems was an imbalance of predictive events                               are essentially independent."
"with parameters (too few events, too many                                         Success in machine learning is not always"
"parameters), overtraining, and a lack of external                            guaranteed. As with any method, a good"
"validation or testing. Nevertheless, among the                               understanding of the problem and an appreciation"
"better designed and better validated studies it was                          of the limitations of the data is important. So too"
"clear that machine learning methods, relative to                             is an understanding of the assumptions and"
"simple statistical methods, could substantially (15-                         limitations of the algorithms being applied. If a"
"25%) improve the accuracy of cancer susceptibility                           machine learning experiment is properly designed,"
"and cancer outcome prediction. In other words,                               the learners correctly implemented and the results"
"machine learning has an important role to play in cancer                     robustly validated, then one usually has a good chance"
"prediction and prognosis.                                                    at success. Obviously if the data is of poor quality,"
"the result will be of poor quality (garbage in = garbage"
"Machine Learning Methods                                                     out). Likewise if there are more variables than events"
"Before beginning with a detailed analysis of what                            to predict then it is also possible to create a series of"
"machine learning methods work best for which                                 redundant learners. This is a set of learning algorithms"
""
""
"Cancer Informatics 2006: 2                                                                                                         61"
"Cruz and Wishart"
""
""
"that seems to perform at the same (low) level regardless        learning and 3) reinforcement learning. They are"
"of the choice of input data. The problem of too many            essentially classified on the basis of desired outcome"
"variables and too few examples is called the “curse of          of the algorithm (Mitchell, 1997; Duda et al. 2001)."
"dimensionality” (Bellman 1961). This curse is not               In supervised learning algorithms a “prescient provider”"
"restricted to machine learning. It also affects many            or teacher gives the learning algorithm a labeled set of"
"statistical methods as well. The only solution is to            training data or examples. These labeled examples"
"reduce the number of variables (features) or increase           are the training set that the program tries to learn about"
"the number of training examples. As a general rule,             or to learn how to map the input data to the desired"
"the sample-per-feature ratio should always exceed 5:1           output. For instance a labeled training set might be a"
"(Somorjai et al. 2003). Not only is the size of the             set of corrupted images of the number “8” (Figure 2)."
"training set important, so too is the variety of the training   Since all the images are labeled as being the number"
"set. Training examples should be selected to span a             “8” and the desired output is the uncorrupted “8”, the"
"representative portion of the data the learner expects          learner is able to train under the supervision of a teacher"
"to encounter. Training too many times on too few                telling it what it is supposed to find. This is the process"
"examples with too little variety leads to the phenomenon        by which most school children learn. In unsupervised"
"of over-training or simply training on noise (Rodvold           learning, a set of examples are given, but no labels are"
"et al. 2001). An over-trained learner, just like an over-       provided. Instead it is up to the learner to find the"
"tired student, will generally perform poorly when it tries      pattern or discover the groups. This is somewhat"
"to process or classify novel data.                              analogous to the process by which most graduate"
"Sometimes conventional statistics proves to be              students learn. Unsupervised learning algorithms"
"more powerful or more accurate than machine                     include such methods as self-organizing feature maps"
"learning. In these cases the user ’s initial                    (SOMs), hierarchical clustering and K-means"
"determinations about the interdependence and                    clustering algorithms. These approaches create"
"nonlinearity of the data would have been wrong.                 clusters from raw, unlabeled or unclassified data. These"
"This is not necessarily a weakness to machine                   clusters can be used later to develop classification"
"learning, it is just a matter of choosing the right             schemes or classifiers."
"tool for the right job. Likewise, not all machine                   The SOM approach (Kohonen 1982) is a"
"learning methods are created equal. Some are                    specialized form of a neural network or ANN. It"
"better for certain kinds of problems while others               is based on using a grid of artificial neurons whose"
"are better for other kinds of problems. For instance            weights are adapted to match input vectors in a"
"some machine learning algorithms scale nicely to                training set. In fact, the SOM was originally"
"the size of the biological domains, others do not.              designed to model biological brain function"
"Likewise some methods may have assumptions or                   (Kohonen 1982). A SOM begins with a set of"
"data requirements that render them inapplicable                 artificial neurons, each having its own physical"
"to the problem at hand. Knowing which method                    location on the output map, which take part in a"
"is best for a given problem is not inherently                   winner-take-all process (a competitive network)"
"obvious. This is why it is critically important to              where a node with its weight vector closest to the"
"try more than one machine learning method on any                vector of inputs is declared the winner and its"
"given training set.                Another common               weights are adjusted making them closer to the input"
"misunderstanding about machine learning is that                 vector. Each node has a set of neighbors. When this"
"the patterns a machine learning tool finds or the               node wins a competition, the neighbors’ weights are"
"trends it detects are non-obvious or not intrinsically          also changed, albeit to a lesser extent. The further the"
"detectable. On the contrary, many patterns or                   neighbor is from the winner, the smaller its weight"
"trends could be detected by a human expert – if                 change. This process is then repeated for each input"
"they looked hard enough at the data. Machine learning           vector for a large number of cycles. Different inputs"
"simply saves on the time and effort needed to discover          produce different winners. The net result is a SOM"
"the pattern or to develop the classification scheme.            which is capable of associating output nodes with"
"Recall that with any interesting discovery, it is frequently    specific groups or patterns in the input data set."
"obvious to the casual observer – particularly after the             Interestingly, almost all machine learning algorithms"
"discovery has been made.                                        used in cancer prediction and prognosis employ"
"There are three general types of machine learning           supervised learning. Furthermore, most of these"
"algorithms: 1) supervised learning; 2) unsupervised             supervised learning algorithms belong to a specific"
""
"62                                                                                                Cancer Informatics 2006: 2"
"                                                                                                Cancer prediction and prognosis"
""
""
""
""
"nodes"
""
""
""
""
"Training                           Layer 1 Layer 2                                 Output"
"Set"
"Figure 2. An example of how a machine learner is trained to recognize images using a training set (a corrupted image of the number"
"“8”) which is labeled or identified as the number “8”."
""
""
"category of classifiers that classify on the basis of             connected to neuron 1, 5, 6 and 8, etc.). This weight"
"conditional probabilities or conditional decisions. The           matrix is called a layer, in analogy to the cortical layers"
"major types of conditional algorithms include: 1)                 in the brain. Neural networks typically use multiple"
"artificial neural networks (ANN – Rummelhart et al.               layers (called hidden layers) to process their input and"
"1986); 2) decision trees (DT – Quinlan, 1986); 3)                 generate an output (Figure 2). To comply with the"
"genetic algorithms (GA – Holland 1975); 4) linear                 mathematical structure of each layer, input and output"
"discriminant analysis (LDA) methods; 5) k-nearest                 data is normally structured as a string, or vector, of"
"neighbor algorithms prognosis with more than 820 of               numbers. One of the challenges in using ANNs is"
"1585 surveyed papers using or referring to ANNs.                  mapping how the real-world input/output (an image, a"
"First developed by McCulloch and Pitts (1943) and                 physical characteristic, a list of gene names, a"
"later popularized in the 1980’s by Rumelhart et al.               prognosis) can be mapped to a numeric vector. In"
"(1986), ANNs are capable of handling a wide range                 ANNs the adjustment of neural connection strengths"
"of classification or pattern recognition problems. Their          is usually done via an optimization technique called"
"strength lies in being able to perform a range of                 back-propagation (short for backwards propagation"
"statistical (linear, logistic and nonlinear regression) and       of errors – Rumelhart et al. 1986). This is a derivative-"
"logical operations or inferences (AND, OR, XOR,                   based process that compares the output of one layer"
"NOT, IF-THEN) as part of the classification process               to the preceding layer’s table. In very simple terms"
"(Rodvold et al. 2001; Mitchell 1997). ANNs were                   the answers or labeled training data are used to"
"originally designed to model the way the brain works              progressively modify the numbers in the neural"
"with multiple neurons being interconnected to each                network’s weight matrices. A learning or information-"
"other through multiple axon junctions. Just as with               transfer function (usually a sigmoidal curve) that is easily"
"biological learning, the strength of the neural                   differentiable is required for back propagation. Most"
"connections is strengthened or weakened through                   ANNs are structured using a multi-layered feed-"
"repeated training or reinforcement on labeled training            forward architecture, meaning they have no feedback,"
"data. Mathematically, these neural connections can                or no connections that loop. The design and structure"
"be represented as a wiring table or matrix (i.e. neuron           of an ANN must be customized or optimized for each"
"1 is connected to neuron 2, 4 and 7; neuron 2 is                  application. Simply choosing a generic ANN"
""
""
""
"Cancer Informatics 2006: 2                                                                                                    63"
"Cruz and Wishart"
""
""
""
"Lump detected"
"by self exam"
""
"No                                Yes"
""
""
"Self exam                                  Mammogram"
"in 1 month                                   suspicious"
""
"No                                Yes"
""
""
"No biopsy                                  Biopsy shows"
"malignancy"
""
"No                                Yes"
""
""
"Non-malignant                               Lumpectomy +"
"cyst                                       Chemo"
"Figure 3. An example of a simple decision tree that might be used in breast cancer diagnosis and treatment. This is an example of a"
"tree that might be formulated via expert assessment. Similar tree structures can be generated by decision tree learners."
""
""
"architecture or naively structuring a generic input/output         by progressively splitting the labeled training data into"
"schema can lead to very poor performance or extremely              subsets based on a numerical or logical test (Quinlan"
"slow training. Another disadvantage of ANNs is the                 1986). This process is repeated on each derived"
"fact that they are a “black-box” technology. Trying to             subset in a recursive manner until further splitting is"
"figure out why an ANN didn’t work or how it performs               either not possible, or a singular classification is"
"its classification is almost impossible to discern. In             achieved. Decision trees have many advantages: they"
"other words, the logic of a trained ANN is not easy to             are simple to understand and interpret, they require"
"decipher.                                                          little data preparation, they can handle many types of"
"In contrast to ANNs, the logic of decision trees               data including numeric, nominal (named) and"
"(DTs) is very easy to discern. Formally a decision                 categorical data, they generate robust classifiers, they"
"tree is a structured graph or flow chart of decisions              are quick to “learn” and they can be validated using"
"(nodes) and their possible consequences (leaves                    statistical tests. However DTs do not generally perform"
"or branches) used to create a plan to reach a goal                 as well as ANNs in more complex classification"
"(Quinlan, 1986; Mitchell 1997). Decision trees                     problems (Atlas et al. 1990)."
"have been around for centuries (especially in                           A somewhat newer machine learning technique is"
"taxonomy) and are a common component to many                       called a support vector machine or SVM (Vapnik,"
"medical diagnostic protocols. An outline of a                      1982; Cortes and Vapnik 1995; Duda et al. 2001)."
"simple decision tree for breast cancer diagnosis is given          SVMs are well known in the world of machine learning"
"in Figure 3. Normally decision trees are designed                  but almost unknown in the field of cancer prediction"
"through consultation with experts and refined through              and prognosis (see Table 2). How an SVM works"
"years of experience or modified to comply with                     can best be understood if one is given a scatter plot of"
"resource limitations or to limit risk. However decision            points, say of tumor mass versus number of axillary"
"tree learners also exist which can automatically                   metastases (for breast cancer) among patients with"
"construct decision trees given a labeled set of training           excellent prognoses and poor prognoses (Figure 4)."
"data. When decision tree learners are used to classify             Two clusters are obviously evident. What the SVM"
"data the leaves in the tree represent classifications and          machine learner would do is find the equation for a"
"branches represent conjunctions of features that lead              line that would separate the two clusters maximally. If"
"to those classifications. A decision tree can be learned           one was plotting more variables (say volume,"
""
"64                                                                                                      Cancer Informatics 2006: 2"
"                                                                                              Cancer prediction and prognosis"
""
""
"metastases and estrogen receptor content) the line of               engineering, computing and physico-chemical"
"separation would become a plane. If more variables                  literature), CiteSeer (computing literature), Google and"
"were included the separation would be defined by a                  Google Scholar (web-accessible scientific literature)."
"hyperplane. The hyperplane is determined by a subset                Query terms included “cancer and machine learning”,"
"of the points of the two classes, called support vectors.           “cancer prediction and machine learning”, “cancer"
"Formally, the SVM algorithm creates a hyperplane that               prognosis and machine learning”, “cancer risk"
"separates the data into two classes with the maximum                assessment and machine learning” as well as multiple"
"margin – meaning that the distance between the                      sub-queries with specific types of machine learning"
"hyperplane and the closest examples (the margin) is                 algorithms. The relevance of the individual papers was"
"maximized. SVMs can be used to perform non-linear                   assessed by reading the titles and abstracts and"
"classification using what is called a non-linear kernel.            identifying papers that used recognizable machine"
"A non-linear kernel is a mathematical function that                 learning methods as well as molecular, clinical,"
"transforms the data from a linear feature space to a                histological, physiological or epidemiological data in"
"non-linear feature space. Applying different kernels                carrying out a cancer prognosis or prediction. Papers"
"to different data sets can dramatically improve the                 that focused on diagnoses or simple tumor"
"performance of an SVM classifier. Like ANNs, SVMs                   classifications were excluded as were papers that had"
"can be used in a wide range of pattern recognition and              coincidental appearances of the words “machine” or"
"classification problems ranging from hand writing                   “learning” in their abstracts. A PubMed search of"
"analysis, speech and text recognition, protein function             “cancer and machine learning” yielded 1585 results,"
"prediction and medical diagnosis (Duda et al. 2001).                while searches of “cancer prediction and machine"
"SVMs are particularly well suited to non-linear                     learning” and “cancer prognosis and machine learning”"
"classification problems, as are k-nearest neighbor                  yielded 174 and 240 hits respectively. A detailed"
"approaches (see Table 1).                                           review of these abstracts led to the identification of"
"103 relevant papers of which 71 could be accessed"
"A Survey of Machine Learning                                        through various library holdings. Using CiteSeer, a"
"Applications in Cancer                                              search with the terms “cancer and machine learning”"
"yielded 349 results, of which 12 (3.4%) were deemed"
"Prediction                                                          relevant to cancer prognosis. Using Google Scholar,"
"In preparing this review several electronic databases               a search using “cancer prognosis and ‘machine"
"were accessed including PubMed (biomedical                          learning’” yielded 996 results, of which 49 (4.9%) were"
"literature), the Science Citation Index (biomedical,                judged relevant to cancer prognosis. Many of these"
"papers were previously identified in the PubMed"
"searches as were the vast majority of the hits in the"
"Science Citation Index searches. From the initial group"
"of papers identified from these electronic searches,"
"their reference lists were further consulted to identify"
"additional papers of interest or relevance. In the end"
"more than 120 relevant papers, going as far back as"
"1989, were identified. Of these, 79 papers could be"
"accessed from existing library holdings and were"
"selected for more detailed analysis (Table 2). While it"
"is impossible to be certain that we achieved complete"
"coverage of all literature on machine learning and"
"cancer prediction/prognosis, we believe that a"
"significant portion of the relevant literature has been"
"assessed for this review."
"From our analysis of the literature several trends"
"were noted. As has been remarked previously, the"
"Figure 4. A simplified illustration of how an SVM might work in     use of machine learning in cancer prediction and"
"distinguishing between basketball players and weightlifters using   prognosis is growing rapidly, with the number of papers"
"height/weight support vectors. In this simple case the SVM has"
"identified a hyperplane (actually a line) which maximizes the       increasing by 25% per year (Figure 1). While it is"
"separation between the two clusters."
""
""
""
"Cancer Informatics 2006: 2                                                                                                65"
"Cruz and Wishart"
""
""
"Table 1. Summary of benefits, assumptions and limitations of different machine learning algorithms"
"Machine"
"Learning"
"Algorithm        Benefits                         Assumptions and/or Limitations"
"Decision Tree      • easy to understand              • classes must be mutually exclusive"
"(Quinlan 1986)       and efficient training          • final decision tree dependent upon order of"
"algorithm                         attribute selection"
"• order of training instances     • errors in training set can result in overly complex"
"has no effect on training         decision trees"
"• pruning can deal with the       • missing values for an attribute make it"
"problem of overfitting            unclear about which branch to take when that"
"attribute is tested"
"Naïve Bayes       • foundation based on              • assumes attributes are statistically"
"(Langley et al      statistical modelling              independent*"
"1992)             • easy to understand               • assumes normal distribution on numeric"
"and efficient training             attributes"
"algorithm                        • classes must be mutually exclusive"
"• order of training instances      • redundant attributes mislead classification"
"has no effect on training        • attribute and class frequencies affect"
"• useful across multiple             accuracy"
"domains"
"k-Nearest         • fast classification of           • slower to update concept description"
"Neighbour           instances                        • assumes that instances with similar attributes will"
"(Patrick &        • useful for non-linear              have similar classifications"
"Fischer 1970;       classification problems          • assumes that attributes will be equally relevant"
"Aha 1992)         • robust with respect to           • too computationally complex as number of attributes"
"irrelevant or novel                increases"
"attributes"
"• tolerant of noisy instances"
"or instances with missing"
"attribute values"
"• can be used for both"
"regression and classification"
"Neural Network    • can be used for                  • difficult to understand structure of algorithm"
"(Rummelhart et al   classification or regression     • too many attributes can result in overfitting"
"1986)             • able to represent Boolean        • optimal network structure can only be determined by"
"functions (AND, OR, NOT)           experimentation"
"• tolerant of noisy inputs"
"• instances can be classified"
"by more than one output"
"Support Vector    • models nonlinear class           • training is slow compared to Bayes and Decision"
"Machine             boundaries                         Trees"
"(Vapnik 1982;     • overfitting is unlikely to       • difficult to determine optimal parameters when"
"Russell and         occur                              training data is not linearly separable"
"Norvig, p 749-52) • computational complexity         • difficult to understand structure of algorithm"
"reduced to quadratic"
"optimization problem"
"• easy to control complexity"
"of decision rule and"
"frequency of error"
"Genetic           • simple algorithm, easy to        • computation or development of scoring function is"
"Algorithm            implement                            non trivial"
"(Holland 1975)     • can be used in feature          • not the most efficient method to find some optima,"
"classification and feature           tends to find local optima rather than global"
"selection                       • complications involved in the representation of"
"• primarily used in                    training/output data"
"optimization"
"• always finds a “good”"
"solution (not always the"
"best solution)"
""
""
"clear that machine learning applications in cancer          In particular, we looked at the frequency with which"
"prediction and prognosis are growing, so too is the         “cancer prediction prognosis methods” and “cancer"
"use of standard statistically-based predictive methods.     risk assessment prediction methods” occurred in"
""
"66                                                                                           Cancer Informatics 2006: 2"
"                                                                                       Cancer prediction and prognosis"
""
""
"PubMed. These queries yielded 1061 and 157 hits              assays (with the possible exception of microarray"
"respectively, giving a non-overlapping set of 1174           data), we believe the results from these studies should"
"papers. Removing the 53 papers with machine learning         be more easily or robustly transferable to other clinical"
"components in this set, we were left with 1121 papers.       settings."
"While a detailed review of each abstract was not                 As seen in Figure 5, there is strong bias among"
"possible, a random sampling indicated that ~80% of           scientists to use machine learning towards predicting"
"these papers were relevant (890 papers) in that they         outcomes or risks associated with breast (24%) and"
"used statistical approaches to predict or prognosticate      prostate (20%) cancer. This, no doubt, reflects the"
"cancer outcomes. Therefore these data suggest that           higher frequency of these cancers among patients in"
"machine learning methods account for 103/890 (11%)           Europe and North America. Nevertheless, machine"
"of all PubMed papers describing cancer prediction or         learning methods appear to have been successfully used"
"prognosis methodology. Overall, the same yearly              in predicting outcomes or risks in nearly a dozen"
"growth trends (i.e. near exponential) in prediction and      different kinds of cancer. This suggests that machine"
"prognosis were observed for the statistical methods          learning methods can be quite generally applied to"
"as for the machine learning methods.                         cancer prediction and prognosis. Figure 5 also"
"When looking at the types of predictions or              illustrates the distribution of the types of machine"
"prognoses being made, the vast majority (86%) are            learning methods applied to different kinds of cancer"
"associated with predicting cancer mortality (44%) and        predictions. Almost 70% of all reported studies use"
"cancer recurrence (42%). However, a growing                  neural networks as their primary (and sometimes only)"
"number of more recent studies are now aimed at               predictor. Support vector machines are a distant"
"predicting the occurrence of cancer or the risk factors      second with 9%, while clustering and decision trees"
"associated with developing cancer. As a general rule,        each account for about 6%. Genetic algorithms and"
"regardless of the machine learning method used, the          other methods (naïve Bayes, fuzzy logic) are rarely"
"type of prediction being made or the type of cancer          used (Table 2). This is both surprising and a bit"
"being evaluated, machine learning methods appear to          disappointing. ANNs are relatively old machine"
"improve the accuracy of predictions by and average           learning technologies which yield so-called “black-box”"
"of 15-25% over alternative or conventional                   results. That is, their performance and classification"
"approaches (Table 2).                                        processes are not easily explained or rationalized. The"
"In assessing how these predictions were made it          existence of other methods (SVMs, DTs, NBs) which"
"appears that the majority (53%) studies relied on            inherently provide easily accessible explanations"
"clinical (cancer staging, cellular histology, nuclear        appears not to be widely known among cancer"
"markers) or demographic data (age, weight, smoking)          informaticians. Overall, many of the papers reviewed"
"– either alone or in combination with other molecular        for this survey were of generally high quality. Some of"
"biomarkers. While histological data is generally more        the better papers are discussed in more detail under"
"accessible, the ambiguity or pathologist-specific            the “Case Studies” section of this review. However, a"
"peculiarities of many histopathological assessments          disturbing number of studies lacked sufficient internal"
"almost always makes it difficult to generalize or transfer   or external validation, were trained on far too few"
"a machine learning tool trained on this kind of data to      examples, tested on only a single machine learner or"
"other clinical settings. Given the limitations of using      had no well-defined standard with which to compare"
"histological assessments in machine learning, there is       the performance of the reported algorithm. These"
"an encouraging trend among more recent studies to            problems are discussed in more detail under the section"
"use more robustly measurable features such as specific       entitled “Limitations and Lessons”."
"protein markers, gene mutations and gene expression"
"values as input data. Approximately 47% of studies           Case Study 1 – Cancer Risk or"
"used this molecular (i.e. proteomic or genomic) data         Susceptibility Prediction"
"either alone (25%) or in combination (22%) with              Of the 79 papers surveyed in this review, relatively"
"clinical data. Given the precision of most molecular         few papers (just 3) employed machine learning to"
""
""
""
"Cancer Informatics 2006: 2                                                                                         67"
"Cruz and Wishart"
""
""
"30%                                                                                                              falling victim to the “curse of dimensionality” (Bellman"
"Naïve Bayes"
"Genetic Algorithm"
"1961; Somorjai et al. 2003). Once the sample size"
"25%"
"Fuzzy Logic"
"Clustering"
"was reduced, several machine learning techniques were"
"Decision Tree"
"SVM"
"employed including a naïve Bayes model, several"
"20%                                                                                  ANN                         decision tree models and a sophisticated support vector"
"Percentage of Papers"
""
""
""
""
"machine (SVM). The SVM and naïve Bayes classifiers"
"15%                                                                                                              attained the highest accuracy using only a set of 3 SNPs"
"and the decision tree classifier attained the highest"
"10%                                                                                                              accuracy using a set of 2 SNPs. The SVM classifier"
"performed the best with an accuracy of 69%, while"
"5%                                                                                                               the naïve Bayes and decision tree classifiers achieved"
"accuracies of 67% and 68%, respectively. These"
"0%                                                                                                               results are approximately 23-25% better than chance."
"Another notable feature to this study was the extensive"
"lung"
""
""
""
"lymphoma"
"liver"
"colorectal"
""
""
""
""
"throat"
"bladder"
""
""
""
""
"other"
"skin"
"breast"
""
""
""
""
"prostate"
""
""
""
""
"level of cross validation and confirmation performed."
"The predictive power of each model was validated in"
"Type of Cancer"
""
"Figure 5. A histogram showing the frequency with which different"
"types of machine learning methods are used to predict different                                                                         at least three ways. Firstly, the training of the models"
"types of cancer. Breast and prostate cancer dominate, however"
"a good range of cancers from different organs or tissues also"
"were assessed and monitored with 20-fold cross-"
"appear to be compatible with machine learning prognoses. The                                                                            validation. A bootstrap resampling method was"
"“other” cancers include brain, cervical, esophageal, leukemia,                                                                          employed by performing the cross-validation 5 times"
"head, neck, ocular, osteosarcoma, pleural mesothelioma, thoracic,"
"thyroid, and trophoblastic (uterine) malignancies. Figure 1                                                                             and averaging the results so as to minimize the stochastic"
"element involved with partitioning of the samples."
"Secondly, to minimize the bias in feature selection (i.e."
"selecting the most informative subset of SNPs), the"
"selection process was performed within each fold for"
"predict cancer risk susceptibility. One of the more                                                                                     a total of 100 times (5 times for each of the 20 folds)."
"interesting papers (Listgarten et al. 2004), used single                                                                                Finally, the results were compared against a random"
"nucleotide polymorphism (SNP) profiles of steroid                                                                                       permutation test which at best, had a predictive"
"metabolizing enzymes (CYP450s) to develop a method                                                                                      accuracy of 50%. While the authors attempted to"
"to retrospectively predict the occurrence of                                                                                            minimize the stochastic element involved with"
"“spontaneous” breast cancer. Spontaneous or non-                                                                                        partitioning of the samples, a better method may have"
"familial breast cancer accounts for about 90% of all                                                                                    been to use leave-one-out cross-validation which"
"breast cancers (Dumitrescu and Cotarla 2005). The                                                                                       would have removed this stochastic element"
"hypothesis in this study was that certain combinations                                                                                  completely. That being said, the multiple cross-"
"of steroid-metabolism gene SNPs would lead to the                                                                                       validations resulted in a standard deviation that was"
"increased accumulation of environmental toxins or                                                                                       not more than 4% for any of the reported accuracies"
"hormones in breast tissue leading to a higher risk for                                                                                  and since all the methods performed close to 25%"
"breast cancer. The authors collected SNP data (98                                                                                       better than chance, this standard deviation is deemed"
"SNPs from 45 different cancer-associated genes) for                                                                                     negligible. While no external validation set was"
"63 patients with breast cancer and 74 patients without                                                                                  reported in this study, we have recently learned that"
"breast cancer (control). Key to the success of this                                                                                     the results described in this paper have been duplicated"
"study was the fact that the authors employed                                                                                            with a similar follow-on study of another 200 individuals"
"several methods to reduce the sample-per-feature                                                                                        (S. Damaraju, personal communication). Overall, this"
"ratio and investigated multiple machine learning                                                                                        study nicely illustrates how the proper design, careful"
"methods to find an optimal classifier. Specifically,                                                                                    implementation, appropriate data selection and"
"from a starting set of 98 SNPs the authors quickly                                                                                      thorough validation of multiple machine learners can"
"reduced this set to just 2-3 SNPs that seemed                                                                                           produce a robust and accurate cancer-risk prediction"
"maximally informative. This reduced the sample-                                                                                         tool. It also highlights how machine learning can reveal"
"per-feature ratio to a respectable 45:1 (for 3 SNPs)                                                                                    important insights into the biology and polygenic risk"
"and 68:1 (for 2 SNPs) instead of close to 3:2 (had all                                                                                  factors associated with spontaneous or non-familial"
"98 SNPs been used). This allowed the study to avoid                                                                                     breast cancer."
""
""
"68                                                                                                                                                                       Cancer Informatics 2006: 2"
"                                                                                        Cancer prediction and prognosis"
""
""
"Case Study 2: Prediction of Cancer                           evident that the authors were aware of this issue and"
"went to considerable lengths to justify their approach"
"Survivability                                                by explaining, in detail, the inner workings of their"
"Nearly half of all machine learning studies on cancer        classifier. This included a description of how the"
"prediction were focused on predicting patient                Bayesian classifier was built, how the EFuNN works,"
"survivability (either 1 year or 5 year survival rates).      and how the two classifiers work together to give a"
"One paper of particular interest (Futschik et al. 2003)      single prediction. In addition, the authors also"
"used a hybrid machine learning approach to predict           investigated, and subsequently confirmed, the"
"outcomes for patients with diffuse large B-cell              independence of the microarray data from the clinical"
"lymphoma (DLBCL). Specifically, both clinical and            data. This attention to detail is particularly exemplary"
"genomic (microarray) data were combined to create            for a machine learning investigation of this kind. This"
"a single classifier to predict survival of DLBCL patients.   study nicely demonstrates how the power of using both"
"This approach differs somewhat from the study of             clinical and genomic data in cancer prognosis can"
"Listgarten et al. (2004) which only employed genomic         substantially enhance prediction accuracy."
"(SNP) data in its classifier schema. Futschik et al."
"hypothesized, correctly, that clinical information could"
"enrich microarray data such that a combined predictor        Case Study 3: Prediction of Cancer"
"would perform better than a classifier based on either       Recurrence"
"microarray data alone or clinical data alone. In             A total of 43% of the studies analyzed for this review"
"assembling the test and training samples, the authors        applied machine learning towards the prediction of"
"collected microarray expression data and clinical            cancer relapse or recurrence. One particularly good"
"information for 56 DLBCL patients. The clinical              example is the study of De Laurentiis et al. (1999),"
"information was obtained from the International              which actually addresses some of the drawbacks noted"
"Prediction Index (IPI) which consists of a set of risk       in the previous studies. These authors aimed to predict"
"factors, that when properly assessed, allows patients        the probability of relapse over a 5 years period for"
"to be separated into groups ranging from low-risk to         breast cancer patients. A combination of 7 prognostic"
"high-risk. The data from the patient’s IPI classifications   variables was used including clinical data such as patient"
"was then used to create a simple Bayesian classifier.        age, tumor size, and number of axillary metastases."
"This classifier achieved an accuracy of 73.2% in             Protein biomarker information such as estrogen and"
"predicting the mortality of DLBCL patients. Separately       progesterone receptor levels was also included. The"
"from the Bayesian classifier, several different types of     aim of the study was to develop an automatic,"
"“evolving fuzzy neural network” (EFuNN) classifiers          quantitative prognostic method that was more reliable"
"were also developed to handle the genomic data. The          than the classical tumor-node-metastasis (TNM)"
"best EFuNN classifier used a subset of 17 genes from         staging system. TNM is a physician-based expert"
"the microarray data. This optimal EFuNN had an               system that relies heavily on the subjective opinion of"
"accuracy of 78.5%. The EFuNN classifier and the              a pathologist or expert clinician. The authors employed"
"Bayesian classifier were then combined into a                an ANN-based model that used data from 2441 breast"
"hierarchical modular system to generate a consensus          cancer patients (times 7 data points each) yielding a"
"prediction. This hybrid classifier attained an accuracy      data set with more than 17,000 data points. This"
"of 87.5%, a clear improvement over the performance           allowed the authors to maintain a sample-to-feature"
"of either classifier alone. This was also 10% better         ratio of well over the suggested minimum of 5 (Somorjai"
"than the best performing machine learning classifier         et al. 2003). The entire data set was partitioned into"
"(77.6% by SVMs).                                             three equal groups: training (1/3), monitoring (1/3),"
"The EFuNN classifier was validated using a leave-        and test sets (1/3) for optimization and validation. In"
"one-out cross-validation strategy. This was likely due       addition, the authors also obtained a separate set of"
"to the small sample size. As with Case Study #1, no          310 breast cancer patient samples from a different"
"external validation set was available to test the            institution, for external validation. This allowed the"
"generality of the model. With only 56 patients (samples)     authors to assess the generalizability of their model"
"being classified via 17 gene features, the sample per        outside their institution — a process not done by the"
"feature ratio (SFR) is just over 3. As a rule, an SFR of     two previously discussed studies."
"less than 5 does not necessarily guarantee a robust              This study is particularly notable not only for the"
"classifier (Somorjai et al. 2003). However, it is quite      quantity of data and the thoroughness of validation,"
""
""
"Cancer Informatics 2006: 2                                                                                          69"
"Cruz and Wishart"
""
""
"but also for the level of quality assurance applied to     obviously many other examples of equally good studies"
"the data handling and processing. For instance, the        with equally impressive results (see Table 2)."
"data was separately entered and stored in a relational     However, it is also important to note that not all"
"database and all of it was independently verified by       machine learning studies are conducted with the same"
"the referring physicians to maintain quality. With 2441    rigor or attention to detail as with these case studies."
"patients and 17,000 data points in the data set, the       Being able to identify potential problems in either the"
"sample size was sufficiently large that a normal           experimental design, validation or learner"
"population distribution of breast cancer patients could    implementation is critical not only for those wishing to"
"be assumed within the data set, even after partitioning.   use machine learning, but also for those needing to"
"Regardless, the authors explicitly verified this           evaluate different studies or to assess different machine"
"assumption by looking at the distribution of the           learning options."
"data for the patients within each set (training,               One of the most common problems seen among"
"monitoring, test, and external) and showed that the        the studies surveyed in this review was the lack of"
"distributions were relatively similar. This quality        attention paid to data size and learner validation."
"assurance and attention to detail allowed the              In other words, there are a number of studies with"
"authors to develop a very accurate and robust              sloppy experimental design. A minimum"
"classifier.                                                requirement for any machine learning exercise is"
"Since the aim of the study was to develop a            having a sufficiently large data set that can be"
"model that predicted relapse of breast cancer better       partitioned into disjoint training and test sets or"
"than the classical TNM staging system, it was              subjected to some reasonable form of n-fold cross-"
"important for the ANN model to be compared to              validation for smaller data sets. Typically 5-fold"
"TNM staging predictions. This was done by comparing        (iteratively taking 20% of the training data out to serve"
"the performance using a receiver operator                  as testing data) or 10-fold cross-validation (iteratively"
"characteristic (ROC) curve. The ANN model (0.726)          taking 10% of the training data out to serve as testing"
"was found to outperform the TNM system (0.677) as          data) is sufficient to validate most any learning"
"measured by the area under the ROC curve. This study       algorithm. This kind of rigorous internal validation is"
"is an excellent example of a well-designed and well-       critical to creating a robust learner that can consistently"
"tested application of machine learning. A sufficiently     handle novel data. Beyond the standard practice of"
"large data set was obtained and data for each sample       internal validation, it is particularly beneficial to perform"
"was independently verified for quality assurance and       a validation test using an external data source. External"
"accuracy. Furthermore, blinded sets for validation were    validation is an important “sanity” check and it also"
"available from both the original data set and from an      helps to catch or minimize any bias that may be"
"external source to assess the generality of the machine    imposed by site or person-specific clinical"
"learning model. Finally, the accuracy of the model was     measurement practices. Of course, this external"
"explicitly compared to that of a classical prognostic      validation set must also be of sufficiently large size to"
"scheme, TNM staging. Perhaps the one drawback to           ensure reproducibility."
"this study was the fact that the authors only tested a         As has been frequently noted before, the size of a"
"single kind of machine learning (ANN) algorithm.           given training set has several implications pertaining to"
"Given the type and quantity of data used, it is quite      robustness, reproducibility and accuracy. The first"
"possible that their ANN model may have been                implication is that for a smaller sample size, almost"
"outperformed by another machine learning technique.        any model is prone to overtraining. Overtraining can"
"lead to reported accuracies that may be misleading or"
"erroneous. For instance, one early study reported only"
"Lessons, Limitations and                                   a single misclassification during the training and testing"
"Recommendations                                            of an ANN for predicting the survival of"
"The 3 case studies outlined in the preceding pages         hepatectomized patients using 9 separate features"
"are just a few examples of how well-designed               (Hamamoto et al. 1995). However, the entire data set"
"machine learning experiments should be conducted           (training and testing) consisted of just 58 patients. This"
"and how the methods and results should be                  particular study then used an external data set to"
"described, validated and assessed – especially in          validate the model where the authors prospectively"
"cancer prediction and prognosis. There are                 predicted the survival outcome with 100% accuracy."
""
""
""
"70                                                                                            Cancer Informatics 2006: 2"
"                                                                                          Cancer prediction and prognosis"
""
""
"However, the external test set only consisted of 11               Just as data quality is important so too is feature"
"patients. The fact that 100% accuracy is attained for a       quality. Certainly the subset of features chosen to train"
"prospective prediction is impressive, but given the size      a model could mean the difference between a robust,"
"of the validation set and the small sample-per-feature        accurate model and one that is flawed and inaccurate."
"ratio, some doubt may be cast on the robustness of            Ideally features should be chosen that are reproducible"
"the predictor. Certainly a larger validation set would        and precisely measurable from one lab (or clinic) to"
"be desirable to reinforce the claim of 100% accuracy.         the next. One study (Delen et al. 2005) used “primary"
"In another example, only 28 cases were used to build          site code” and “site specific surgery code” as features"
"an ANN for predicting throat cancer recurrence that           to predict breast cancer survivability. While these"
"made use of the expression levels of 60 genes from            clinical features may be helpful in determining the"
"microarray data (Kan et al. 2004). The accuracy of            outcome for breast cancer patients at this particular"
"the model was claimed to be 86%, but this is                  hospital, for this moment in time, they may become"
"particularly suspect given the very small sample size.        irrelevant over time. Even worse, if new site codes or"
"Indeed it is quite likely that this ANN was over-trained.     site specific surgery codes are created, the model will"
"The size of a given data set also significantly affects   have to be re-trained to account for the new codes."
"the sample-per-feature ratio. As a rule, the sample-          Similar feature selection problems often occur with"
"per-feature ratio should be at least 5-10 (Somorjai et        histological assessments. As good as many"
"al. 2003). Small sample-per-feature ratios are a              pathologists are there is always some inconsistency"
"particularly big problem for microarray studies, which        (up to 30% in many cases) between different"
"often have thousands of genes (ie features), but only         histopathological assessments from different sites or"
"hundreds of samples. The study by Ohira et al. (2005)         different pathologists. As a rule, the best features are"
"provides one such example of the problems one may             those that are highly reproducible, universal or absolute"
"encounter trying to process too much microarray data.         (age, gender, weight, certain biomarker measurements,"
"These authors created a probabilistic output statistical      etc). Even with these seemingly robust features it is"
"classifier to predict prognosis of neuroblastoma patients     important to remember that clinical data sets are not"
"using microarray data from 136 tumor samples. Each            static entities. With time the importance or relevance"
"microarray had 5340 genes, leading to a sample-per-           of these clinical measures may evolve over time with"
"feature ratio of ~0.025. A sample-per-feature ratio           some features being added, modified or deleted."
"this small is highly susceptible to the problems of           Therefore a classifier must also be able to adapt to"
"overtraining. Furthermore, with a sample-per-feature          different feature sets over time too."
"ratio of this size it is also possible to develop highly          Another important lesson that was learned from"
"redundant classification models which perform equally         assessing many of these machine learning papers was"
"well despite being trained on different subsets of genes.     the value of using multiple predictor models based on"
"The problem with redundant models is that the                 different machine learning techniques. While ANNs"
"robustness of any one model cannot be guaranteed as           are often considered to be very sophisticated and"
"more test cases become available.                             advanced machine learning methods, ANNs are not"
"Data size is not the only limitation for effective        always the best tools for the job. Sometimes simpler"
"machine learning. Data set quality and careful feature        machine learning methods, like the naïve Bayes and"
"selection are also equally important (recall: “garbage        decision tree methods can substantially outperform"
"in=garbage out”). For large data sets data entry and          ANNs (Delen et al. 2005). Assessing the performance"
"data verification are of paramount importance. Often          of a machine learning predictor against other predictors"
"careless data entry can lead to simple off-by-one errors      is critical to choosing the optimal tool. It is also critical"
"in which all the values for a particular variable are         to deciding if the method is any better than previously"
"shifted up or down by one row in a table. This is why         existing schemes. Ideally, any newly published"
"independent verification by a second data-entry               machine learning model should be compared against"
"curator or data checker is always beneficial. Further         either another kind of learning model, a traditional"
"verification or spot checking of data integrity by a          statistical model or an expert-based prognostic scheme"
"knowledgeable expert, not just a data entry clerk, is         such as the TNM staging system. As seen in Table 2,"
"also a valuable exercise. Unfortunately, the methods          sometimes the more sophisticated machine learning"
"employed to ensure data quality and integrity are rarely      methods do not lead to the best predictors. In some"
"discussed in most machine learning papers.                    cases, traditional statistics actually outperform machine"
""
""
"Cancer Informatics 2006: 2                                                                                              71"
"Cruz and Wishart"
""
""
"Table 2: Survey of machine learning methods used in cancer prediction showing the types of cancer, clinical"
"endpoints, choice of algorithm, performance and type of training data."
"Machine                       Improve-"
"Cancer        Clinical          Learning                      ment     Training"
"Type          Endpoint          Algorithm Benchmark           (%)      Data          Reference"
"bladder        recurrence       fuzzy logic   statistics     16         mixed         Catto et al, 2003"
"bladder        recurrence       ANN           N/A            N/A        clinical      Fujikawa et al, 2003"
"bladder        survivability    ANN           N/A            N/A        clinical      Ji et al, 2003"
"bladder        recurrence       ANN           N/A            N/A        clinical      Spyridonos et al, 2002"
"brain          survivability    ANN           statistics     N/A        genomic       Wei et al, 2004"
"breast         recurrence       clustering    statistics     N/A        mixed         Dai et al, 2005"
"breast         survivability    decision      statistics     4          clinical      Delen et al, 2005"
"tree"
"breast         susceptibility   SVM           random         19         genomic       Listgarten et al, 2004"
"breast         recurrence       ANN           N/A            N/A        clinical      Mattfeldt et al, 2004"
"breast         recurrence       ANN           N/A            N/A        mixed         Ripley et al, 2004"
"breast         recurrence       ANN           statistics     1          clinical      Jerez-Aragones et al,"
"2003"
"breast         survivability    ANN           statistics     N/A        clinical      Lisboa et al, 2003"
"breast         treatment        ANN           N/A            N/A        proteomic     Mian et al, 2003"
"response"
"breast         survivability    clustering    statistics     0          clinical      Seker et al, 2003"
"breast         survivability    fuzzy logic   statistics     N/A        proteomic     Seker et al, 2002"
"breast         survivability    SVM           N/A            N/A        clinical      Lee et al, 2000"
"breast         recurrence       ANN           expert         5          mixed         De Laurentiis et al,"
"1999"
"breast         survivability    ANN           statistics     1          clinical      Lundin et al, 1999"
"breast         recurrence       ANN           statistics     23         mixed         Marchevsky et al, 1999"
"breast         recurrence       ANN           N/A            N/A        clinical      Naguib et al, 1999"
"breast         survivability    ANN           N/A            N/A        clinical      Street, 1998"
"breast         survivability    ANN           expert         5          clinical      Burke et al, 1997"
"breast         recurrence       ANN           statistics     N/A        mixed         Mariani et al, 1997"
"breast         recurrence       ANN           expert         10         clinical      Naguib et al, 1997"
"cervical       survivability    ANN           N/A            N/A        mixed         Ochi et al, 2002"
"colorectal     recurrence       ANN           statistics     12         clinical      Grumett et al, 2003"
"colorectal     survivability    ANN           statistics     9          clinical      Snow et al, 2001"
"colorectal     survivability    clustering    N/A            N/A        clinical      Hamilton et al, 1999"
"colorectal     recurrence       ANN           statistics     9          mixed         Singson et al, 1999"
"colorectal     survivability    ANN           expert         11         clinical      Bottaci et al, 1997"
"esophageal     treatment        SVM           N/A            N/A        proteomic     Hayashida et al, 2005"
"response"
"esophageal     survivability    ANN           statistics     3          clinical      Sato et al, 2005"
"leukemia       recurrence       decision      N/A            N/A        proteomic     Masic et al, 1998"
"tree"
"liver          recurrence       ANN           statistics     25         genomic       Rodriguez-Luna et al,"
"2005"
"liver          recurrence       SVM           N/A            N/A        genomic       Iizuka et al, 2003"
"liver          susceptibility   ANN           statistics     -2         clinical      Kim et al, 2003"
"liver          survivability    ANN           N/A            N/A        clinical      Hamamoto et al, 1995"
"lung           survivability    ANN           N/A            N/A        clinical      Santos-Garcia et al,"
"2004"
"lung           survivability    ANN           statistics     9          mixed         Hanai et al, 2003"
"lung           survivability    ANN           N/A            N/A        mixed         Hsia et al, 2003"
"lung           survivability    ANN           statistics     N/A        mixed         Marchevsky et al, 1998"
"lung           survivability    ANN           N/A            N/A        clinical      Jefferson et al, 1997"
"lymphoma       survivability    ANN           statistics     22         genomic       Ando et al, 2003"
"lymphoma       survivability    ANN           expert         10         mixed         Futschik et al, 2003"
"lymphoma       survivability    ANN           N/A            N/A        genomic       O’Neill and Song, 2003"
"lymphoma       survivability    ANN           expert         N/A        genomic       Ando et al, 2002"
"lymphoma       survivability    clustering    N/A            N/A        genomic       Shipp et al, 2002"
"head/neck      survivability    ANN           statistics     11         clinical      Bryce et al, 1998"
""
"72                                                                                      Cancer Informatics 2006: 2"
"                                                                                     Cancer prediction and prognosis"
""
""
"Table 2: Continued."
"Machine                       Improve-"
"Cancer        Clinical         Learning                      ment     Training"
"Type          Endpoint         Algorithm Benchmark           (%)      Data               Reference"
"neck          treatment        ANN           N/A             N/A         clinical        Drago et al, 2002"
"response"
"ocular        survivability    SVM           N/A             N/A         genomic         Ehlers and Harbour,"
"2005"
"osteo-     treatment           SVM           N/A             N/A         genomic         Man et al, 2005"
"sarcoma    response"
"pleural    survivability       clustering    N/A             N/A         genomic         Pass et al, 2004"
"mesothelioma"
"prostate   treatment           ANN           N/A             N/A         mixed           Michael et al, 2005"
"response"
"prostate   recurrence          ANN           statistics      0           clinical        Porter et al, 2005"
"prostate   treatment           ANN           N/A             N/A         clinical        Gulliford et al, 2004"
"response"
"prostate   recurrence          ANN           statistics      16          mixed           Poulakis et al, 2004a"
"prostate   recurrence          ANN           statistics      11          mixed           Poulakis et al, 2004b"
"prostate   recurrence          SVM           statistics      6           clinical        Teverovskiy et al, 2004"
"prostate   recurrence          ANN           statistics      0           clinical        Kattan, 2003"
"prostate   recurrence          genetic       N/A             N/A         mixed           Tewari et al, 2001"
"algorithm"
"prostate      recurrence       ANN           statistics      0           clinical        Ziada et al, 2001"
"prostate      susceptibility   decision      N/A             N/A         clinical        Crawford et al, 2000"
"tree"
"prostate      recurrence       ANN           statistics      13          clinical        Han et al, 2000"
"prostate      treatment        ANN           N/A             N/A         proteomic       Murphy et al, 2000"
"response"
"prostate      recurrence       naïve         statistics      1           clinical        Zupan et al, 2000"
"Bayes"
"prostate      recurrence       ANN           N/A             N/A         clinical        Mattfeldt et al, 1999"
"prostate      recurrence       ANN           statistics      17          clinical        Potter et al, 1999"
"prostate      recurrence       ANN           N/A             N/A         mixed           Naguib et al, 1998"
"skin          survivability    ANN           expert          14          clinical        Kaiserman et al, 2005"
"skin          recurrence       ANN           expert          27          proteomic       Mian et al, 2005"
"skin          survivability    ANN           expert          0           clinical        Taktak et al, 2004"
"skin          survivability    genetic       N/A             N/A         clinical        Sierra and Larranga,"
"algorithm                                                 1998"
"stomach       recurrence       ANN           expert          28          clinical        Bollschweiler et al,"
"2004"
"throat        recurrence       fuzzy logic N/A               N/A         clinical        Nagata et al, 2005"
"throat        recurrence       ANN           statistics      0           genomic         Kan et al, 2004"
"throat        survivability    decision tree statistics      N/A         proteomic       Seiwerth et al, 2000"
"thoracic      treatment        ANN           N/A             N/A         clinical        Su et al, 2005"
"response"
"thyroid       survivability    decision      statistics      N/A         clinical        Kukar et al, 1997"
"tree"
"tropho-       survivability    genetic       N/A             N/A         clinical        Marvin et al, blastic"
"algorithm                                                 1999"
""
""
"learning methods (Kaiserman et al. 2005; Kim et al.       hypothesis, it follows defined procedures and it"
"2003). Unfortunately, only about 17% of the papers        requires data to be validated. Because machine"
"reviewed here tested more than one machine learning       learners represent true experimental procedures, they"
"classifier.                                               should be treated as such. Therefore detailed"
"It is also important to remember that the machine      methodological documentation is of paramount"
"learning process is essentially a computational           importance. Ideally, the data sets used for training and"
"experiment. Like any experiment it is based on a          testing should be described in detail and made"
""
""
"Cancer Informatics 2006: 2                                                                                       73"
"Cruz and Wishart"
""
""
"available to the public. Information about training and"
"testing data should also be well-described including"
"References"
"the way in which the sets were partitioned. Likewise         Those papers marked with a “*” represent papers"
"the details regarding the algorithms used and their          of good general interest or relevance, those marked"
"implementations should be provided or recorded to            with a “**” represent papers of exceptional interest"
"permit others to verify and reproduce the results. In        or relevance."
"Aha D. 1992. Tolerating noisy, irrelevant and novel attributes in"
"principle, the results from a good machine learning              instance-based learning algorithms. International Journal of Man-"
"experiment should be as reproducible as any other                Machine Studies, 36:267-287."
"standard lab protocol.                                       Ando T, Suguro M, Hanai T, et al. 2002. Fuzzy neural network"
"applied to gene expression profiling for predicting the prognosis"
"of diffuse large B-cell lymphoma. Jpn J Cancer Res, 93:1207-"
"12"
"Ando T, Suguro M, Kobayashi T, et al. 2003. Multiple fuzzy neural"
"Conclusion                                                       network system for outcome prediction and classification of"
"In this review we have attempted to explain,                     220 lymphoma patients on the basis of molecular profiling."
"compare and assess the performance of different                  Cancer Sci, 94:906-13."
"Atlas L, Cole R, Connor J, et al. 1990. Performance comparisons"
"machine learning that are being applied to cancer                between backpropagation networks and classification trees on"
"prediction and prognosis. Specifically we                        three real-world applications. Advances in Neural Inf. Process."
"identified a number of trends with respect to the                Systems, 2:622-629."
"Bach PB, Kattan MW, Thornquist MD, et al. 2003. Variations in"
"types of machine learning methods being used, the                lung cancer risk among smokers. J Natl Cancer Inst, 95:470-8."
"types of training data being integrated, the kinds           Baldus SE, Engelmann K, Hanisch FG. 2004. MUC1 and the MUCs:"
"of endpoint predictions being made, the types of                 a family of human mucins with impact in cancer biology. Crit"
"cancers being studied and the overall performance                Rev Clin Lab Sci, 41:189-231."
"Bellman R. 1961. Adaptive Control Processes: A Guided Tour,"
"of these methods in predicting cancer susceptibility             Princeton University Press."
"or outcomes. While ANNs still predominate it is              Bocchi L, Coppini G, Nori J, Valli G. 2004. Detection of single and"
"evident that a growing variety of alternate machine              clustered microcalcifications in mammograms using fractals"
"models and neural networks. Med Eng Phys, 26:303-12."
"learning strategies are being used and that they are         Bollschweiler EH, Monig SP, Hensler K, et al. 2004. Artificial neural"
"being applied to many types of cancers to predict                network for prediction of lymph node metastases in gastric"
"at least three different kinds of outcomes. It is                cancer: a phase II diagnostic study. Ann Surg Oncol, 11:506-11."
"*Bottaci L, Drew PJ, Hartley JE, et al. 1997. Artificial neural"
"also clear that machine learning methods generally               networks applied to outcome prediction for colorectal cancer"
"improve the performance or predictive accuracy of                patients in separate institutions. Lancet, 350:469-72."
"most prognoses, especially when compared to                  Bryce TJ, Dewhirst MW, Floyd CE Jr, et al. 1998. Artificial neural"
"conventional statistical or expert-based systems.                network model of survival in patients treated with irradiation"
"with and without concurrent chemotherapy for advanced"
"While most studies are generally well constructed                carcinoma of the head and neck. Int J Radiat Oncol Biol Phys,"
"and reasonably well validated, certainly greater                 41:239-45."
"attention to experimental design and                         Burke HB, Bostwick DG, Meiers I, et al. 2005. Prostate cancer"
"outcome: epidemiology and biostatistics. Anal Quant Cytol Histol,"
"implementation appears to be warranted, especially               27:211-7."
"with respect to the quantity and quality of biological       *Burke HB, Goodman PH, Rosen DB, et al. 1997. Artificial neural"
"data. Improvements in experimental design along with             networks improve the accuracy of cancer survival prediction."
"Cancer, 79:857-62."
"improved biological validation would no doubt enhance        Catto JW, Linkens DA, Abbod MF, et al. 2003. Artificial intelligence"
"the overall quality, generality and reproducibility of           in predicting bladder cancer outcome: a comparison of neuro-"
"many machine-based classifiers. Overall, we believe              fuzzy modeling and artificial neural networks. Clin Cancer Res,"
"that if the quality of studies continues to improve, it is       9:4172-7."
"*Cicchetti DV. 1992. Neural networks and diagnosis in the clinical"
"likely that the use of machine learning classifier will          laboratory: state of the art. Clin Chem, 38:9-10."
"become much more commonplace in many clinical and            Claus EB. 2001. Risk models used to counsel women for breast and"
"hospital settings.                                               ovarian cancer: a guide for clinicians. Fam Cancer, 1:197-206."
"Cochran AJ. 1997. Prediction of outcome for patients with cutaneous"
"melanoma. Pigment Cell Res, 10:162-7."
"*Colozza M, Cardoso F, Sotiriou C, et al. 2005. Bringing molecular"
"Acknowledgements                                                 prognosis and prediction to the clinic. Clin Breast Cancer, 6:61-"
"76"
"The authors wish to thank Genome Prairie, a division         Cortes C, Vapnik V. 1995. Support-vector networks. Machine"
"of Genome Canada, as well as the National Research               Learning, 20:273-297."
"Crawford ED, Batuello JT, Snow P, et al. 2000. The use of artificial"
"Council’s National Institute for Nanotechnology                  intelligence technology to predict lymph node spread in men"
"(NINT) for their financial support.                              with clinically localized prostate carcinoma. Cancer, 88:2105-"
"9"
""
""
"74                                                                                                   Cancer Informatics 2006: 2"
"                                                                                                             Cancer prediction and prognosis"
""
"Dai H, van’t Veer L, Lamb J, et al. 2005. A cell proliferation signature"
"Pass HI, Liu Z, Wali A, et al. 2004. Gene expression profiles predict"
"is a marker of extremely poor outcome in a subpopulation of                survival and progression of pleural mesothelioma. Clin Cancer"
"breast cancer patients. Cancer Res, 65:4059-66."
"Res, 10:849-59."
"**De Laurentiis M, De Placido S, Bianco AR, et al. 1999. A prognostic"
"Patrick EA, Fischer FP. 1970. A generalized k-nearest neighbor rule."
"model that makes quantitative estimates of probability of relapse          Information and Control 16:128-152."
"for breast cancer patients. Clin Cancer Res, 5:4133-9."
"Petricoin EF, Liotta LA. 2004. SELDI-TOF-based serum proteomic"
"*Delen D, Walker G, Kadam A. 2005. Predicting breast cancer"
"pattern diagnostics for early detection of cancer. Curr Opin"
"survivability: a comparison of three data mining methods. Artif            Biotechnol, 15:24-30."
"Intell Med, 34:113-27."
"Piccart M, Lohrisch C, Di Leo A, et al. 2001. The predictive value of"
"Dettling M. 2004. BagBoosting for tumor classification with gene"
"HER2 in breast cancer. Oncology, 61 Suppl 2:73-82."
"expression data. Bioinformatics, 20:3583-93.                           Porter CR, Gamito EJ, Crawford ED. 2005. Model to predict prostate"
"Domchek SM, Eisen A, Calzone K, et al. 2003. Application of breast"
"biopsy outcome in large screening population with independent"
"cancer risk prediction models in clinical practice. J Clin Oncol,"
"validation in referral setting. Urology, 65:937-41."
"21:593-601.                                                            *Potter SR, Miller MC, Mangold LA, et al. 1999. Genetically"
"Drago GP, Setti E, Licitra L, et al. 2002. Forecasting the performance"
"engineered neural networks for predicting prostate cancer"
"status of head and neck cancer patient treatment by an interval"
"progression after radical prostatectomy. Urology, 54:791-5."
"arithmetic pruned perceptron. IEEE Trans Biomed Eng, 49:782-           Poulakis V, Witzsh U, de Vries R, et al. 2004a. Preoperative neural"
"7"
"network using combined magnetic resonance imaging variables,"
"Duda RO, Hart PE, Stork DG. (2001) Pattern classification (2nd"
"prostate specific antigen, and Gleason score to predict prostate"
"edition). New York: Wiley.                                                 cancer recurrence after radical prostatectomy. Eur Urol, 46:571-"
"Duffy MJ. 2001. Biochemical markers in breast cancer: which ones"
"8"
"are clinically useful? Clin Biochem, 34:347-52."
"Poulakis V, Witzsch U, de Vries, et al. 2004b. Preoperative neural"
"*Duffy MJ. 2005. Predictive markers in breast and other cancers: a             network using combined magnetic resonance imaging variables,"
"review. Clin Chem, 51:494-503."
"prostate-specific antigen, and gleason score for predicting prostate"
"Dumitrescu RG, Cotarla I. 2005.Understanding breast cancer risk —"
"cancer biochemical recurrence after radical prostatectomy."
"where do we stand in 2005? J Cell Mol Med, 9:208-21.                       Urology, 64:1165-70."
"Ehlers JP, Harbour JW. 2005. NBS1 expression as a prognostic marker"
"Quinlan J.R. 1986. Induction of decision trees. Machine Learning,"
"in uveal melanoma. Clin Cancer Res, 11:1849-53."
"1:81-106."
"Fielding LP, Fenoglio-Preiser CM, Freedman LS. 1992. The future            Ripley RM, Harris AL, Tarassenko L. 2004. Non-linear survival"
"of prognostic factors in outcome prediction for patients with"
"analysis using neural networks. Stat Med, 23:825-42."
"cancer. Cancer, 70:2367-77."
"Rodriguez-Luna H, Vargas HE, Byrne T, et al. 2005. Artificial neural"
"Fujikawa K, Matsui Y, Kobayashi T, et al. 2003. Predicting disease             network and tissue genotyping of hepatocellular carcinoma in"
"outcome of non-invasive transitional cell carcinoma of the"
"liver-transplant recipients: prediction of recurrence."
"urinary bladder using an artificial neural network model: results"
"Transplantation, 79:1737-40."
"of patient follow-up for 15 years or longer. Int J Urol, 10:149-       **Rodvold DM, McLeod DG, Brandt JM, et al. 2001. Introduction"
"52"
"to artificial neural networks for physicians: taking the lid off the"
"**Futschik ME, Sullivan M, Reeve A, et al. 2003. Prediction of"
"black box. Prostate, 46:39-44."
"clinical behaviour and treatment for cancers. Appl Bioinformatics,     Rumelhart DE, Hinton GE, Williams RJ. 1986. Learning"
"2(3 Suppl):S53-8."
"representations by back-propagating errors. Nature, 323:533-6."
"Gascon F, Valle M, Martos R, et al. 2004. Childhood obesity and"
"Russell SJ, Norvig P. 2003. Artificial Intelligence: A Modern Approach."
"hormonal abnormalities associated with cancer risk. Eur J Cancer           2nd ed. New Jersey: Pearson Education, Inc. p 733-59."
"Prev, 13:193-7."
"Santos-Garcia G, Varela G, Novoa N, et al. 2004. Prediction of"
"Grumett S, Snow P, Kerr D. 2003. Neural networks in the prediction"
"postoperative morbidity after lung resection using an artificial"
"of survival in patients with colorectal cancer. Clin Colorectal            neural network ensemble. Artif Intell Med, 30:61-9."
"Cancer, 2:239-44."
"Sato F, Shimada Y, Selaru FM, et al. 2005. Prediction of survival in"
"Gulliford SL, Webb S, Rowbottom CG, et al. 2004. Use of artificial"
"patients with esophageal carcinoma using artificial neural"
"neural networks to predict biological outcomes for patients                networks. Cancer, 103:1596-605."
"receiving radical radiotherapy of the prostate. Radiother Oncol,"
"Savage KJ, Gascoyne RD. 2004. Molecular signatures of lymphoma."
"71:3-12."
"Int J Hematol, 80:401-9."
"**Hagerty RG, Butow PN, Ellis PM, et al. 2005. Communicating               Seiwerth S, Stambuk N, Konjevoda P, et al. 2000."
"prognosis in cancer care: a systematic review of the literature."
"Immunohistochemical analysis and prognostic value of cathepsin"
"Ann Oncol, 16:1005-53."
"D determination in laryngeal squamous cell carcinoma. J Chem"
"*Hamamoto I, Okada S, Hashimoto T, et al. 1995. Prediction of the              Inf Comput Sci, 40:545-9."
"early prognosis of the hepatectomized patient with hepatocellular"
"*Seker H, Odetayo MO, Petrovic D, et al. 2002. Assessment of"
"carcinoma with a neural network. Comput Biol Med, 25:49-59."
"nodal involvement and survival analysis in breast cancer patients"
"Hamilton PW, Bartels PH, Anderson N, et al. 1999. Case-based                   using image cytometric data: statistical, neural network and fuzzy"
"prediction of survival in colorectal cancer patients. Anal Quant"
"approaches. Anticancer Res, 22:433-8."
"Cytol Histol, 21:283-91."
"Seker H, Odetayo MO, Petrovic D, et al. 2003. A fuzzy logic based-"
"Han M, Snow PB, Epstein JI, et al. 2000. A neural network predicts             method for prognostic decision making in breast and prostate"
"progression for men with gleason score 3+4 versus 4+3 tumors"
"cancers. IEEE Trans Inf Technol Biomed, 7:114-22."
"after radical prostatectomy. Urology, 56:994-9."
"Shipp MA, Ross KN, Tamayo P, et al. 2002. Diffuse large B-cell"
"Hanai T, Yatabe Y, Nakayama Y, et al. 2003. Prognostic models in               lymphoma outcome prediction by gene-expression profiling and"
"patients with non-small-cell lung cancer using artificial neural"
"supervised machine learning. Nat Med, 8:68-74."
"networks in comparison with logistic regression. Cancer Sci,"
"94:473-7."
""
""
""
""
"Cancer Informatics 2006: 2                                                                                                                     75"
"Cruz and Wishart"
""
"Hayashida Y, Honda K, Osaka Y, et al. 2005. Possible prediction of              Marchevsky AM, Patel S, Wiley KJ, et al. 1998. Artificial neural"
"chemoradiosensitivity of esophageal cancer by serum protein profiling.        networks and logistic regression as tools for prediction of survival"
"Clin Cancer Res, 11:8042-7.                                                   in patients with Stages I and II non-small cell lung cancer. Mod"
"Holland JH. 1975. Adaptation in Natural and Artificial Systems. University         Pathol, 11:618-25."
"of Michigan Press, Ann Arbor                                               Marchevsky AM, Shah S, Patel S. 1999. Reasoning with uncertainty in"
"Hsia TC, Chiang HC, Chiang D. 2003. Prediction of survival in surgical             pathology: artificial neural networks and logistic regression as tools"
"unresectable lung cancer by artificial neural networks including              for prediction of lymph node status in breast cancer patients. Mod"
"genetic polymorphisms and clinical parameters. J Clin Lab Anal,               Pathol, 12:505-13."
"17:229-34.                                                                 Mariani L, Coradini D, Biganzoli E, et al. 1997. Prognostic factors for"
"*Iizuka N, Oka M, Yamada-Okabe H, et al. 2003. Oligonucleotide                     metachronous contralateral breast cancer: a comparison of the linear"
"microarray for prediction of early intrahepatic recurrence of                 Cox regression model and its artificial neural network extension."
"hepatocellular carcinoma after curative resection. Lancet, 361:923-           44:167-78."
"9.                                                                         Marvin N, Bower M, Rowe JE. 1999. An evolutionary approach to"
"Jefferson MF, Pendleton N, Lucas SB, et al. 1997. Comparison of a genetic          constructing prognostic models. Artif Intell Med, 15:155-65."
"algorithm neural network with logistic regression for predicting           Masic N, Gagro A, Rabatic S, et al. 1998. Decision-tree approach to the"
"outcome after surgery for patients with nonsmall cell lung carcinoma.         immunophenotype-based prognosis of the B-cell chronic lymphocytic"
"Cancer, 79:1338-42.                                                           leukemia. Am J Hematol, 59:143-8."
"Jerez-Aragones JM, Gomez-Ruiz JA, Ramos-Jimenez G, et al. 2003. A               Mattfeldt T, Kestler HA, Hautmann R, et al. 1999. Prediction of prostatic"
"combined neural network and decision trees model for prognosis of             cancer progression after radical prostatectomy using artificial neural"
"breast cancer relapse. Artif Intell Med, 27:45-63.                            networks: a feasibility study. BJU Int, 84:316-23."
"Ji W, Naguib RN, Ghoneim MA. 2003. Neural network-based assessment              Mattfeldt T, Kestler HA, Sinn HP. 2004. Prediction of the axillary lymph"
"of prognostic markers and outcome prediction in bilharziasis-                 node status in mammary cancer on the basis of clinicopathological"
"associated bladder cancer. IEEE Trans Inf Technol Biomed, 7:218-              data and flow cytometry. Med Biol Eng Comput, 42:733-9."
"24.                                                                        McCarthy JF, Marx KA, Hoffman PE, et al. 2004. Applications of machine"
"*Kaiserman I, Rosner M, Pe’er J. 2005. Forecasting the prognosis of                learning and high-dimensional visualization in cancer detection,"
"choroidal melanoma with an artificial neural network. Ophthalmology,          diagnosis, and management. Ann N Y Acad Sci, 1020:239-62"
"112:1608.                                                                  McCulloch W, Pitts W. 1943. A logical calculus of the ideas imminent in"
"*Kan T, Shimada Y, Sato F, et al. 2004. Prediction of lymph node metastasis        nervous activity. Bull Math Biophys, 5: 115-33."
"with use of artificial neural networks based on gene expression profiles   Mian S, Ball G, Hornbuckle F, et al. 2003. A prototype methodology"
"in esophageal squamous cell carcinoma. Ann Surg Oncol, 11:1070-8.             combining surface-enhanced laser desorption/ionization protein chip"
"Kattan MW. 2003. Comparison of Cox regression with other methods for               technology and artificial neural network algorithms to predict the"
"determining prediction models and nomograms. J Urol, 170:S6-9.                chemoresponsiveness of breast cancer cell lines exposed to Paclitaxel"
"*Kim YS, Sohn SY, Kim DK, et al. 2003. Screening test data analysis for            and Doxorubicin under in vitro conditions. Proteomics, 3:1725-37."
"liver disease prediction model using growth curve. Biomed                  Mian S, Ugurel S, Parkinson E, et al. 2005. Serum proteomic fingerprinting"
"Pharmacother, 57:482-8.                                                       discriminates between clinical stages and predicts disease progression"
"Kohonen T. 1982. Self-organized formation of topologically correct feature         in melanoma patients. J Clin Oncol, 23:5088-93."
"maps. Biol Cybernetics, 43:59-69.                                          Michael A, Ball G, Quatan N, et al. 2005. Delayed disease progression"
"Kukar M, Besic N, Konomenko I, et al. 1996. Prognosing the survival                after allogeneic cell vaccination in hormone-resistant prostate cancer"
"time of the patients with the anaplastic thyroid carcinoma with machine       and correlation with immunologic variables. Clin Cancer Res,"
"learning. Proc Intelligent Data Analysis in Medicine and                      11:4469-78."
"Pharmacology, 50-56.                                                       *Mitchell T. 1997. Machine Learning. New York: McGraw Hill."
"Langley P, Iba W, Thompson K. 1992. An analysis of Bayesian classifiers.        Murphy GP, Snow P, Simmons SJ, et al. 2000. Use of artificial neural"
"Proceedings of the Tenth National Conference on Artificial                    networks in evaluating prognostic factors determining the response"
"Intelligence, 223-228.                                                        to dendritic cells pulsed with PSMA peptides in prostate cancer"
"Langley P, Sage S. 1994. Induction of selective Bayesian classifiers.              patients. Prostate, 42:67-72."
"Proceedings of the Tenth Conference on Uncertainty in Artificial           Nagata T, Schmelzeisen R, Mattern D, et al. 2005. Application of fuzzy"
"Intelligence, 399-406.                                                        inference to European patients to predict cervical lymph node"
"Lee Y, Mangasarian OL, Wolberg WH. 2000. Breast cancer survival and                metastasis in carcinoma of the tongue. Int J Oral Maxillofac Surg,"
"chemotherapy: A support vector machine analysis. DIMACS Series                34:138-42."
"in Discrete Mathematics and Theoretical Computer Science, 55:1-            Naguib RN, Adams AE, Horne CH, et al. 1997. Prediction of nodal"
"20.                                                                           metastasis and prognosis in breast cancer: a neural model. Anticancer"
"Leenhouts HP. 1999. Radon-induced lung cancer in smokers and non-                  Res, 17:2735-41."
"smokers: risk implications using a two-mutation carcinogenesis model.      Naguib RN, Robinson MC, Neal DE, et al. 1998. Neural network analysis"
"Radiat Environ Biophys, 1999 38:57-71.                                        of combined conventional and experimental prognostic markers in"
"Lisboa PJ, Wong H, Harris P, et al. 2003. A Bayesian neural network                prostate cancer: a pilot study. Br J Cancer, 78:246-50."
"approach for modelling censored data with an application to prognosis      Naguib RN, Sakim HA, Lakshmi MS, et al. 1999. DNA ploidy and cell"
"after surgery for breast cancer. Artif Intell Med, 28:1-25.                   cycle distribution of breast cancer aspirate cells measured by image"
"**Listgarten J, Damaraju S, Poulin B et al. 2004. Predictive models for            cytometry and analyzed by artificial neural networks for their"
"breast cancer susceptibility from multiple single nucleotide                  prognostic significance. IEEE Trans Inf Technol Biomed, 3:61-9."
"polymorphisms. Clin Cancer Res, 10:2725-37.                                Ochi T, Murase K, Fujii T, et al. 2002. Survival prediction using artificial"
"*Lundin M, Lundin J, Burke HB, et al. 1999. Artificial neural networks             neural networks in patients with uterine cervical cancer treated by"
"applied to survival prediction in breast cancer. Oncology, 57:281-6.          radiation therapy alone. Int J Clin Oncol, 7:294-300."
"Maclin PS, Dempsey J, Brooks J, et al. 1991. Using neural networks to           *Ohira M, Oba S, Nakamura Y, et al. 2005. Expression profiling using a"
"diagnose cancer. J Med Syst, 15:11-9.                                         tumor-specific cDNA microarray predicts the prognosis of"
"Man TK, Chintagumpala M, Visvanathan J, et al. 2005. Expression profiles           intermediate risk neuroblastomas. Cancer Cell, 7:337-50."
"of osteosarcoma that can predict response to chemotherapy. Cancer          O’Neill MC, Song L. 2003. Neural network analysis of lymphoma"
"Res, 65:8142-50.                                                              microarray data: prognosis and diagnosis near-perfect. BMC"
"Bioinformatics, 4:13."
""
""
"76                                                                                                                          Cancer Informatics 2006: 2"
"                                                                                                           Cancer prediction and prognosis"
""
"Sierra B, Larranaga P. 1998. Predicting survival in malignant skin        Teverovskiy M, Kumar V, Ma J, et al. 2004. Improved prediction of"
"melanoma using Bayesian networks automatically induced by                 prostate cancer recurrence based on an automated tissue image"
"genetic algorithms. An empirical comparison between different             analysis system. Proc IEEE Int Symp Biomed Imaging, 257-60."
"approaches. 14:215-30.                                                Tewari A, Issa M, El-Galley R, et al. 2001. Genetic adaptive neural"
"*Simes RJ. 1985. Treatment selection for cancer patients: application         network to predict biochemical failure after radical prostatectomy:"
"of statistical decision theory to the treatment of advanced ovarian       a multi-institutional study. Mol Urol, 5:163-9."
"cancer. J Chronic Dis, 38:171-86.                                     Vapnik V. 1982. Estimation of Dependences Based on Empirical"
"Singson RP, Alsabeh R, Geller SA, et al. 1999. Estimation of tumor            Data. Springer Verlag, New York"
"stage and lymph node status in patients with colorectal               Vendrell E, Morales C, Risques RA, et al. 2005. Genomic determinants"
"adenocarcinoma using probabilistic neural networks and logistic           of prognosis in colorectal cancer. Cancer Lett, 221:1-9."
"regression. Mod Pathol, 12:479-84.                                    Wang JX, Zhang B, Yu JK, et al. 2005. Application of serum protein"
"Snow PB, Kerr DJ, Brandt JM, et al. 2001. Neural network and                  fingerprinting coupled with artificial neural network model in"
"regression predictions of 5-year survival after colon carcinoma           diagnosis of hepatocellular carcinoma. Chin Med J (Engl),"
"treatment. Cancer, 91(8 Suppl):1673-8.                                    118:1278-84."
"**Somorjai RL, Dolenko B, Baumgartner R. 2003. Class prediction           Wei JS, Greer BT, Westermann F, et al. 2004. Prediction of clinical"
"and discovery using gene microarray and proteomics mass                   outcome using gene expression profiling and artificial neural"
"spectroscopy data: curses, caveats, cautions. Bioinformatics,             networks for patients with neuroblastoma. Cancer Res,"
"19:1484-91.                                                               64(19):6883-91. Erratum in: Cancer Res, 65:374."
"Spyridonos P, Cavouras D, Ravazoula P, et al. 2002. A computer-           Weston AD, Hood L. 2004. Systems biology, proteomics, and the"
"based diagnostic and prognostic system for assessing urinary              future of health care: toward predictive, preventative, and"
"bladder tumour grade and predicting cancer recurrence. Med                personalized medicine. J Proteome Res, 3:179-96."
"Inform Internet Med, 27:111-22.                                       Zhou X, Liu KY, Wong ST. 2004. Cancer classification and prediction"
"*Street W. 1998. A neural network model for prognostic prediction.            using logistic regression with Bayesian gene selection. J Biomed"
"Proceedings of the Fifteenth International Conference on                  Inform, 37:249-59."
"Machine Learning, 540-6.                                              Ziada AM, Lisle TC, Snow PB, et al. 2001. Impact of different"
"Su M, Miften M, Whiddon C, et al. 2005. An artificial neural network          variables on the outcome of patients with clinically confined"
"for predicting the incidence of radiation pneumonitis. Med Phys,          prostate carcinoma: prediction of pathologic stage and"
"32:318-25.                                                                biochemical failure using an artificial neural network. Cancer,"
"Taktak AF, Fisher AC, Damato BE. 2004. Modelling survival after               91(8 Suppl):1653-60."
"treatment of intraocular melanoma using artificial neural networks    *Zupan B, Demsar J, Kattan MW, et al. 2000. Machine learning for"
"and Bayes theorem. Phys Med Biol, 49:87-98.                               survival analysis: a case study on recurrence of prostate cancer."
"Artif Intell Med, 20:59-75."
""
""
""
""
"Cancer Informatics 2006: 2                                                                                                                   77"
""
